{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_Writing a training loop from scratch_org.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gXbj-HI7Fo1f",
        "XuCQAP5yGlcN",
        "Y_vzVp4cpVq6",
        "I5tO5_Tf3Beo",
        "KkitgFq33-TC",
        "QMx1p0xK3UI-",
        "zcJWNNPj2fKd",
        "D6l70mrj3zpZ",
        "uN7lCxdR4lyt",
        "NoT8vc3W6vRu",
        "wOvfllpr8ZBf",
        "c3ExNOc2EECF",
        "gvfoxkSDFNs5",
        "o--GC-NsFesB",
        "z1hE05KYJsbB",
        "QQ4h7zwqJsmj",
        "YSsDyLvZLVsR",
        "SOZWbnJmMIrY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H42oySwlAsW_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrting a training loop\n",
        "\n",
        "- If I want to customize the learning algorithm of model while still leveraging the convenience of fit, I can use subclassing Model and implement own train_step() method, which is called repeatedly during fit().\n",
        "\n",
        "- If I want very low-level control over training & evaluation, I shold write my own training and evaluation loops from scratch."
      ],
      "metadata": {
        "id": "gXbj-HI7Fo1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n",
        "- [Intro](https://www.tensorflow.org/guide/autodiff)\n",
        "- [Advanced](https://www.tensorflow.org/guide/advanced_autodiff )\n"
      ],
      "metadata": {
        "id": "XuCQAP5yGlcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-differentiation -> backpropagation\n",
        "# It is so tricky\n",
        "## what is backpropagation\n",
        "- https://brilliant.org/wiki/backpropagation/\n",
        "\n",
        "    Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.\n",
        "\n",
        "\n",
        "- https://en.wikipedia.org/wiki/Backpropagation\n",
        "\n",
        "  In machine learning, backpropagation (backprop,[1] BP) is a widely used algorithm for training feedforward neural networks. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally. These classes of algorithms are all referred to generically as \"backpropagation\".[2] In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single inputâ€“output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.[3]\n",
        "\n",
        "## what is automatic differentiation\n",
        "- https://en.wikipedia.org/wiki/Automatic_differentiation\n",
        "\n",
        "  In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation, computational differentiation,[1][2] auto-differentiation, or simply autodiff, is a set of techniques to evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
        "  Figure 1: How automatic differentiation relates to symbolic differentiation\n",
        "  Automatic differentiation is distinct from symbolic differentiation and numerical differentiation. Symbolic differentiation faces the difficulty of converting a computer program into a single mathematical expression and can lead to inefficient code. Numerical differentiation (the method of finite differences) can introduce round-off errors in the discretization process and cancellation. Both of these classical methods have problems with calculating higher derivatives, where complexity and errors increase. Finally, both of these classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems.\n"
      ],
      "metadata": {
        "id": "Y_vzVp4cpVq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autodiff -> useful to backpropagation.\n",
        "\n",
        "To autodiff, tensorflow remembers the operations in some order in forward pass. \n",
        "Tensorflow records relevant operations executed inside the context of a tf.GradientsTape onto a tape.\n",
        "\n",
        "\n",
        "Tensorflow -> GradientTape for autodiff. = computing the gradient of a computation with respect to some inputs, usually tf.Variables.\n",
        "\n",
        "\n",
        "\n",
        "Tensorflow -> \n"
      ],
      "metadata": {
        "id": "ZR4SI4FxybyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tf.Variable \n",
        "\n",
        "## trainable (default : True)"
      ],
      "metadata": {
        "id": "I5tO5_Tf3Beo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scalar case"
      ],
      "metadata": {
        "id": "KkitgFq33-TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vztyZGOQ2erE",
        "outputId": "a4bf6a1f-3dd1-4e6d-eeab-f0205cfc0247"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Records operation in context of GradientTape"
      ],
      "metadata": {
        "id": "QMx1p0xK3UI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape :\n",
        "  y = x ** 2"
      ],
      "metadata": {
        "id": "knoYjH752fB_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GradientTape.gradient(target,sources) -> calcuate the gradient of some target(often a loss) relative to some source(oftne the model's variables)"
      ],
      "metadata": {
        "id": "zcJWNNPj2fKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx = tape.gradient(y,x)\n",
        "dy_dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn7yPSe12fQP",
        "outputId": "2f5cad53-0d92-4f84-d14e-77ef18ec5a7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More than scalar"
      ],
      "metadata": {
        "id": "D6l70mrj3zpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf.Variable(initial_value = tf.random.normal((3,2))) -> \n",
        "\n",
        "  initial_value\tA Tensor, or Python object convertible to a Tensor, which is the initial value for the Variable. The initial value must have a shape specified unless validate_shape is set to False. Can also be a callable with no argument that returns the initial value when called. In that case, dtype must be specified. (Note that initializer functions from init_ops.py must first be bound to a shape before being used here.)"
      ],
      "metadata": {
        "id": "uN7lCxdR4lyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [tf.reduce_mean()](https://www.tensorflow.org/api_docs/python/tf/Variable)\n",
        "  - Computes the mean of elements across dimensions of a tensor.\n",
        "  - If axis is None, all dimensions are reduced, and a tensor with a single element is returned."
      ],
      "metadata": {
        "id": "NoT8vc3W6vRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with tf.GradientTape(persistent=True) as tape:\n",
        "  - What does \"persistent\" mean ? -> To compute multiple gradients over the same computation, create a persistent gradient tape. "
      ],
      "metadata": {
        "id": "wOvfllpr8ZBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python \n",
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape(persistent=False) as g:\n",
        "  g.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n",
        "print(dz_dx)\n",
        "\n",
        "dy_dx = g.gradient(y, x)\n",
        "print(dy_dx)\n",
        "tf.Tensor(108.0, shape=(), dtype=float32)\n",
        "```\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  RuntimeError                              Traceback (most recent call last)\n",
        "  <ipython-input-39-2429d3837f86> in <module>()\n",
        "        7 print(dz_dx)\n",
        "        8 \n",
        "  ----> 9 dy_dx = g.gradient(y, x)\n",
        "      10 print(dy_dx)\n",
        "\n",
        "  /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\n",
        "    1030     \"\"\"\n",
        "    1031     if self._tape is None:\n",
        "  -> 1032       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n",
        "    1033                          \"compute one set of gradients (or jacobians)\")\n",
        "    1034     if self._recording:\n",
        "\n",
        "  RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n",
        "```"
      ],
      "metadata": {
        "id": "T9P_vSwT9Ym9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [numpy @ operator](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul)\n",
        "\n",
        "-> matrix multiplication"
      ],
      "metadata": {
        "id": "c3ExNOc2EECF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable(tf.random.normal((3,2)),name = 'w')\n",
        "b = tf.Variable(tf.zeros(2,dtype = tf.float32),name = 'b')\n",
        "x = [[1.,2.,3.]]"
      ],
      "metadata": {
        "id": "1Um3io8n3zvG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x@ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ],
      "metadata": {
        "id": "bgvutBPd5gCt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[dl_dw,dl_db] = tape.gradient(loss,[w,b])"
      ],
      "metadata": {
        "id": "466NyAkg6kij"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad = tape.gradient(loss,[w,b])\n",
        "grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1f7gxybC5ty",
        "outputId": "a58c8447-2d3d-4b9e-dba1-8e4342f7c61f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
              " array([[ 2.170581 ,  3.5292852],\n",
              "        [ 4.341162 ,  7.0585704],\n",
              "        [ 6.5117435, 10.587855 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.170581 , 3.5292852], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vars = { 'w' : w,\n",
        "           'b' : b}\n",
        "grad = tape.gradient(loss,my_vars)\n",
        "grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auxR8sfbC-vD",
        "outputId": "fd7629da-4cdf-46c5-8f92-87d997aef1dc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.170581 , 3.5292852], dtype=float32)>,\n",
              " 'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
              " array([[ 2.170581 ,  3.5292852],\n",
              "        [ 4.341162 ,  7.0585704],\n",
              "        [ 6.5117435, 10.587855 ]], dtype=float32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradients with respect to a model\n",
        "\n",
        "- In many cases, I will compute gradients regarding a model's trainable variables.\n",
        "- All subclasses of tf.Module aggregate their variables -> Module.trainable_variables. -> I can use it."
      ],
      "metadata": {
        "id": "G8LuPdopDMIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.Dense(2,activation = 'relu')\n",
        "x = tf.constant([[1.,2.,3.]])"
      ],
      "metadata": {
        "id": "Z1MD76enE9g2"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### forward pass"
      ],
      "metadata": {
        "id": "gvfoxkSDFNs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n"
      ],
      "metadata": {
        "id": "qFkApiuHFGk5"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate gradients\n"
      ],
      "metadata": {
        "id": "o--GC-NsFesB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad = tape.gradient(loss,layer.trainable_variables)\n",
        "grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S660gpl6Fm45",
        "outputId": "7d9a2686-b929-42c5-9ede-6771cee6f63c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
              " array([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controlling what the tape watches\n",
        "\n",
        "- The default behavior is to record all operations after accessing a trainalbe tf.Variable."
      ],
      "metadata": {
        "id": "z1hE05KYJsbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.tensor is not watched. To record gradients with respect to a tf.tensor, I need to call tape.watch()"
      ],
      "metadata": {
        "id": "QQ4h7zwqJsmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A trainable variable"
      ],
      "metadata": {
        "id": "eKyknmcoMLYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = tf.Variable(3.0,name = 'x0')"
      ],
      "metadata": {
        "id": "lHpN54RjJstq"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not trainable"
      ],
      "metadata": {
        "id": "jilzVR2-JtEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = tf.Variable(3.0,name = 'x1',trainable = False)"
      ],
      "metadata": {
        "id": "tfAxtmvnKZ7w"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a Variable : A variable + tensor returns a tensor"
      ],
      "metadata": {
        "id": "zKollUUdKftF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = tf.Variable(2.0,name = 'x2')+ 1.0"
      ],
      "metadata": {
        "id": "LS7SrBcgKrDT"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a variable"
      ],
      "metadata": {
        "id": "_9aw85CYKvE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x3 = tf.constant(3.0,name = 'x3')"
      ],
      "metadata": {
        "id": "hPor_o3BKxly"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x2)\n",
        "  y = (x0**2) + (x1**2) + (x2**2)"
      ],
      "metadata": {
        "id": "9PC94KyZK2IB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad = tape.gradient(y, [x0,x1,x2,x3])"
      ],
      "metadata": {
        "id": "eG4RbTBKK-5d"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in grad :\n",
        "  print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "394e6ga2K_E8",
        "outputId": "743ef1f2-35b8-4ec2-a445-bfb49b43ca8e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradientTape.watched_variables"
      ],
      "metadata": {
        "id": "YSsDyLvZLVsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tape.watched_variables():\n",
        "  print(i.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn_fil4GLZX4",
        "outputId": "123a3f86-f301-4269-9786-480f3f8974e2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disable the default behavior of watching all tf.Variables.\n",
        "\n",
        "- Set watch_accessed_variables =False "
      ],
      "metadata": {
        "id": "SOZWbnJmMIrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(x)\n",
        "\n",
        "grad = tape.gradient(y1, {\"x0\" : x0 , \"x1\" : x1})\n",
        "grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8qhU5KcM6DM",
        "outputId": "978dd874-50e8-4b04-b66c-01ff9dd69eca"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x0': None, 'x1': <tf.Tensor: shape=(), dtype=float32, numpy=0.9999546>}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate results"
      ],
      "metadata": {
        "id": "JGntU0IhPt0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''It can be done\n",
        "\n",
        "x = tf.constant([1,3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y= x*x\n",
        "  z = y*y\n",
        "grad1 = tape.gradient(z,[x,y]) \n",
        "grad1\n",
        "#grad2 = tape.gradient(y,x)\n",
        "'''\n",
        "\n",
        "''' It doesn't work\n",
        "\n",
        "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n",
        "\n",
        "x = tf.constant([1,3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y= x*x\n",
        "  z = y*y\n",
        "grad1 = tape.gradient(z,x) \n",
        "grad2 = tape.gradient(y,x)\n",
        "'''\n",
        "\n",
        "\n",
        "x = tf.constant([1,3.0])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y= x*x\n",
        "  z = y*y\n",
        "grad1 = tape.gradient(z,x) \n",
        "grad2 = tape.gradient(y,x)\n"
      ],
      "metadata": {
        "id": "DZrFbXcLQBqZ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "- There is a tiny overhead associated with doing operations inside a gradient tape context.\n",
        "\n",
        "- Graidient tapes use memory to store intermediate results, including inputs and outputs for use during the backwards pass.\n"
      ],
      "metadata": {
        "id": "uFjUWVgsShy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients of non-scalar targets"
      ],
      "metadata": {
        "id": "qdeATJ7wTBJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x** 2\n",
        "  y1 = 1/x\n",
        "\n",
        "tape.gradient({\"y0\": y0, \"y1\":y1},x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tYzIIk7TBQd",
        "outputId": "6dd609bf-6766-4c9d-f5cb-2f26b5fe0673"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=3.75>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3.,4.]\n",
        "  print(y)\n",
        "tape.gradient(y,x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtJXBP27TBXb",
        "outputId": "19f503bb-c5b8-48eb-d088-f07e729a353b"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([6. 8.], shape=(2,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x = tf.linspace(-10.0,10.0,200+1)\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape :\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "  print(\"y : \",y )\n",
        "dy_dx = tape.gradient(y,x)\n",
        "\n",
        "dy_dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRzOu_oqTBeD",
        "outputId": "e19b51b9-da2e-4225-b770-2e5e1abb0ddf"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y :  tf.Tensor(0.8807971, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.104993574>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control flow\n",
        "\n",
        "- Control flow statements such as If and While can be used in context of gradient tape.\n",
        "- The control statements are not differentiable, so they are invisible to gradient-based optimizers."
      ],
      "metadata": {
        "id": "QDmfTj6kTB3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0 :\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1 ** 2\n",
        "\n",
        "dv0, dv1 = tape.gradient(result,[v0,v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)\n",
        "\n",
        "dx = tape.gradient(result,x)\n",
        "\n",
        "print(dx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmAZopUIhVMc",
        "outputId": "959a66b7-8943-4e02-ba17-d79d5333eeb0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}